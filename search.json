[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Paired residue prediction dependencies in ESM2",
    "section": "",
    "text": "This notebook details a quick analysis we performed on the ESM2 (Evolutionary Scale Modeling) model for masked token prediction. We stumbled upon a counter-intuitive result related to the effect that masking one residue has on the distribution of another.\nBefore jumping into our results, let’s first establish what masked token prediction is, how it works, and our consequent motivation for this analysis."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Paired residue prediction dependencies in ESM2",
    "section": "",
    "text": "This notebook details a quick analysis we performed on the ESM2 (Evolutionary Scale Modeling) model for masked token prediction. We stumbled upon a counter-intuitive result related to the effect that masking one residue has on the distribution of another.\nBefore jumping into our results, let’s first establish what masked token prediction is, how it works, and our consequent motivation for this analysis."
  },
  {
    "objectID": "index.html#brief-primer",
    "href": "index.html#brief-primer",
    "title": "Paired residue prediction dependencies in ESM2",
    "section": "Brief primer",
    "text": "Brief primer\nAt its core, masked token prediction is a fundamental training technique in natural language processing. The basic idea is simple: take a sequence of text, hide some of its tokens (by replacing them with a special “mask” token), and ask the model to predict what should go in those masked positions. This fill-in-the-blanks approach, formally known as masked language modeling (MLM), is remarkably effective at enabling models to learn underlying patterns and dependencies in sequential data1. And in the protein language model space, nearly every state-of-the-art model is trained with masked token prediction.\n1 The BERT paper is the key seminal paper that popularized MLM (Devlin et al., 2019).Once a model is trained on this task, you can mutate a sequence of interest by masking residues and having the model predict what amino acid the mask should be replaced with. This is called unmasking and the procedure goes like this. First, an incomplete sequence is passed through the model. It’s incomplete because at one or more residues, a masked token is placed in lieu of an amino acid. The collection of masked positions is denoted as \\(M\\) and the partially masked sequence is denoted as \\(S_{-M}\\). Here’s an example:\n\\[\nS_{-M} = \\text{A R &lt;mask&gt; A D I &lt;mask&gt;}\n\\]\nHere, \\(M = \\lbrace 2, 6 \\rbrace\\), which is \\(0\\)-indexed because Python has corrupted our minds. When this sequence is passed through a model with an attached language-modelling head (e.g. an ESM2 model), the output is an array of unnormalized probabilities for each amino acid at the masked positions. We call these values logits, which can be readily converted into an amino acid probabilities for each masked residue2. The amino acid probability distribution of the \\(i^\\text{th}\\) residue is denoted as \\(p(i|M)\\), where \\(i \\in M\\).\n2 What are logits? Inside a protein language model, each layer produces neuronal activations as it processes the sequence. These intermediate activations represent increasingly abstract features about the amino acid patterns. The final layer produces specific activations dubbed logits - one logit for each possible amino acid that could appear at a masked position. A high logit value for an amino acid corresponds to a high probability of that amino acid. This blog has more details."
  },
  {
    "objectID": "index.html#motivation",
    "href": "index.html#motivation",
    "title": "Paired residue prediction dependencies in ESM2",
    "section": "Motivation",
    "text": "Motivation\nWith this understanding of masked token prediction in place, we can explore an important tradeoff in how these predictions are made. When performing masked token prediction, there is a balance between efficiency and information utilization. On one extreme, you could take \\(S_{-M}\\), pass it through the model once, and then unmask each masked token. This would be very computationally efficient. However, this approach precludes the model from leveraging the information gained from each newly unmasked position to inform subsequent predictions. At the other end of the spectrum, you could unmask tokens one at a time: predict the first position, update the sequence with that prediction, pass the updated sequence through the model again to predict the second position, and so on. This iterative approach allows for maximum information utilization but can be really expensive in terms of computational resources. There is also a concern that by unmasking individually, you only make conservative choices that prevent a deeper exploration of sequence space.\nAll this is to say that developing effective unmasking strategies is an important concern for maximizing the utility of masked-token prediction models. And to do that, it’s worth gaining some insight into how masking at one position affects the outcome at another masked position. Our analysis approaches this question by masking two locations, both individually and jointly, and quantifying any changes in their predicted amino acid probabilities. Specifically, we’ll consider a pair of positions \\(i\\) and \\(j\\) and ask:\n\nHow does \\(p(i | \\lbrace i \\rbrace)\\) compare with \\(p(i | \\lbrace i, j \\rbrace)\\)?\nHow does \\(p(j | \\lbrace j \\rbrace)\\) compare with \\(p(j | \\lbrace i, j \\rbrace)\\)?\n\nBy doing this repeatedly for each pair of residues in the sequence, will we start to capture interdependencies between the sequence positions? Do these statistical dependencies relate back to things we expect, like the attention map of the model, or perhaps the contact map of the amino acids in 3D space? Are there any interpretable patterns at all? We will assess the similarity of these distributions with Jensen-Shannon divergence to find out."
  },
  {
    "objectID": "index.html#loading-the-model",
    "href": "index.html#loading-the-model",
    "title": "Paired residue prediction dependencies in ESM2",
    "section": "Loading the model",
    "text": "Loading the model\nOur analysis will focus on the ESM2 model series (Lin et al., 2023).\nA good place to start is a quick survey of the available ESM2 model sizes. The ESM2 architecture was created and trained for 6 different model sizes, which are described here:\n\n1from analysis.utils import ModelName\nfrom rich.console import Console\n\nconsole = Console()\n\nfor model_name in ModelName:\n    console.print(model_name, style=\"strong\")\n\n\n1\n\nSource code\n\n\n\n\nModelName.ESM2_8M\n\n\n\nModelName.ESM2_35M\n\n\n\nModelName.ESM2_150M\n\n\n\nModelName.ESM2_650M\n\n\n\nModelName.ESM2_3B\n\n\n\nModelName.ESM2_15B\n\n\n\nAll of these models are hosted on HuggingFace and can be loaded using HuggingFace’s transformers python package.\nThe larger models need GPUs to run. We’ll do that later. For now, let’s load up the smallest model for prototyping the analysis.\n\n\n\n\n\n\nNote\n\n\n\nYou can still follow along on your own computer without GPU access.\n\n\n\nfrom transformers import AutoTokenizer, EsmForMaskedLM\n\nsmall_model = EsmForMaskedLM.from_pretrained(ModelName.ESM2_8M.value)\nconsole.print(small_model)\n\n/opt/miniconda3/envs/paired-token-masking/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning:\n\nIProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n\n\n\nEsmForMaskedLM(\n  (esm): EsmModel(\n    (embeddings): EsmEmbeddings(\n      (word_embeddings): Embedding(33, 320, padding_idx=1)\n      (dropout): Dropout(p=0.0, inplace=False)\n      (position_embeddings): Embedding(1026, 320, padding_idx=1)\n    )\n    (encoder): EsmEncoder(\n      (layer): ModuleList(\n        (0-5): 6 x EsmLayer(\n          (attention): EsmAttention(\n            (self): EsmSelfAttention(\n              (query): Linear(in_features=320, out_features=320, bias=True)\n              (key): Linear(in_features=320, out_features=320, bias=True)\n              (value): Linear(in_features=320, out_features=320, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n              (rotary_embeddings): RotaryEmbedding()\n            )\n            (output): EsmSelfOutput(\n              (dense): Linear(in_features=320, out_features=320, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n          )\n          (intermediate): EsmIntermediate(\n            (dense): Linear(in_features=320, out_features=1280, bias=True)\n          )\n          (output): EsmOutput(\n            (dense): Linear(in_features=1280, out_features=320, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (emb_layer_norm_after): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n    )\n    (contact_head): EsmContactPredictionHead(\n      (regression): Linear(in_features=120, out_features=1, bias=True)\n      (activation): Sigmoid()\n    )\n  )\n  (lm_head): EsmLMHead(\n    (dense): Linear(in_features=320, out_features=320, bias=True)\n    (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n    (decoder): Linear(in_features=320, out_features=33, bias=False)\n  )\n)\n\n\n\nIn addition to the base ESM architecture, this model has an attached language-model decoder head, which you can see at the bottom of the printed model architecture. It is this part of the model that converts the latent embedding produced by the model into the logits array which we will then transform into amino acid probability distributions."
  },
  {
    "objectID": "index.html#getting-the-probability-array-p",
    "href": "index.html#getting-the-probability-array-p",
    "title": "Paired residue prediction dependencies in ESM2",
    "section": "Getting the probability array \\(P\\)",
    "text": "Getting the probability array \\(P\\)\n\n\n\n\n\n\n\nTable of all 33 tokens in the model’s vocabulary and their indices in the logits array.\n\n\n\nToken\n\n\nIndex\n\n\n\n\n\n0\n&lt;cls&gt;\n\n\n1\n&lt;pad&gt;\n\n\n2\n&lt;eos&gt;\n\n\n3\n&lt;unk&gt;\n\n\n4\nL\n\n\n5\nA\n\n\n6\nG\n\n\n7\nV\n\n\n8\nS\n\n\n9\nE\n\n\n10\nR\n\n\n11\nT\n\n\n12\nI\n\n\n13\nD\n\n\n14\nP\n\n\n15\nK\n\n\n16\nQ\n\n\n17\nN\n\n\n18\nF\n\n\n19\nY\n\n\n20\nM\n\n\n21\nH\n\n\n22\nW\n\n\n23\nC\n\n\n24\nX\n\n\n25\nB\n\n\n26\nU\n\n\n27\nZ\n\n\n28\nO\n\n\n29\n.\n\n\n30\n-\n\n\n31\n&lt;null_1&gt;\n\n\n32\n&lt;mask&gt;\n\n\n\n\n\n\nA forward pass of small_model yields a logits array, which we need to transform into our desired array of amino acid probability distributions (\\(P\\)). The transformation requires two key steps. Since we only want amino acid probabilities, we first subset the logits to include just the 20 amino acids from the model’s 33-token vocabulary (shown in the table on the right). And second, we need to apply a softmax to convert these selected logits into probabilities. Here’s the code to perform both steps:\n\nimport pandas as pd\nimport torch\n1from analysis.utils import amino_acids\n\ntokenizer = AutoTokenizer.from_pretrained(\n    ModelName.ESM2_8M.value,\n    clean_up_tokenization_spaces=True,\n)\naa_token_indices = tokenizer.convert_tokens_to_ids(amino_acids)\n\n\ndef get_probs_array(logits):\n    \"\"\"Given a logits array, return an amino acid probability distribution array.\n\n    Args:\n        logits:\n            The output from the language-modelling head of an ESM2 model. Shape is (batch_size, seq_len, vocab_size).\n\n    Returns:\n        A probability array. Shape is (batch_size, seq_len, 20).\n    \"\"\"\n    if not isinstance(logits, torch.Tensor):\n        logits = torch.tensor(logits)\n\n    # Only include tokens that correspond to amino acids. This excludes special, non-amino acid tokens.\n    logits_subset = logits[..., aa_token_indices]\n\n    return torch.nn.functional.softmax(logits_subset, dim=-1).detach().numpy()\n\n\n1\n\nSource code\n\n\n\n\nAs a sanity check, let’s create a sequence of repeating leucines. Then we’ll mask one of the residues, and see what the model predicts at that position. Although there is no theoretically “correct” answer for what the model should predict, given that the entire sequence is homogenously leucine, it would be bizarre if the model predicted anything but.\n\nimport numpy as np\nimport pandas as pd\n\n# Create a sequence of all leucine, then tokenize it.\nsequence = \"L\" * 200\ntokenized = tokenizer([sequence], return_tensors=\"pt\")\n\n# Place a mask token in the middle of the tokenized sequence.\nmask_pos = 100\ntokenized[\"input_ids\"][0, mask_pos] = tokenizer.mask_token_id\n\n# Pass the output of the model through our `get_probs_array` function.\nlogits_array = small_model(**tokenized).logits  # Shape (batch_size=1, seq_len=202, vocab_size=33)\nprobs_array = get_probs_array(logits_array)  # Shape (batch_size=1, seq_len=202, vocab_size=20)\n\n# We are only interested in the position we masked, so let's subset to that.\n# Note that the position is indexed with `mask_pos + 1` because of a preceding \"CLS\"\n# token that prefixes the tokenized sequence.\nprobs_array_at_mask = probs_array[0, mask_pos + 1, :]\n\n# Like a probability distribution should, the elements sum to one (or very nearly)\nassert np.isclose(probs_array_at_mask.sum().item(), 1.0)\n\nprobs = dict(zip(amino_acids, probs_array_at_mask, strict=False))\nprobs = dict(\n    sorted(((aa, prob.item()) for aa, prob in probs.items()), key=lambda x: x[1], reverse=True)\n)\npd.Series(probs).to_frame(name=\"Probability\")\n\n\n\n\n\n\nThe probability masses predicted by the model for the masked residue.\n\n\n\nProbability\n\n\n\n\nL\n0.990615\n\n\nP\n0.001851\n\n\nI\n0.001022\n\n\nS\n0.000988\n\n\nT\n0.000935\n\n\nR\n0.000911\n\n\nA\n0.000728\n\n\nV\n0.000715\n\n\nF\n0.000417\n\n\nQ\n0.000303\n\n\nH\n0.000272\n\n\nY\n0.000250\n\n\nD\n0.000236\n\n\nG\n0.000217\n\n\nM\n0.000147\n\n\nK\n0.000143\n\n\nE\n0.000115\n\n\nN\n0.000060\n\n\nW\n0.000057\n\n\nC\n0.000018\n\n\n\n\n\n\nEncouragingly, the elements sum to one and the model confidently (&gt;99%) predicts leucine."
  },
  {
    "objectID": "index.html#sequence-of-interest",
    "href": "index.html#sequence-of-interest",
    "title": "Paired residue prediction dependencies in ESM2",
    "section": "Sequence of interest",
    "text": "Sequence of interest\nNow let’s introduce a real protein to work with. Any protein will do, really, but we’ve been exploring the use of adenosine deaminase (human) as a template for protein design, so we’ve decided to use that.\n\nfrom pathlib import Path\n\nfrom biotite.sequence.io.fasta import FastaFile\n\nsequence_path = Path(\"input/P00813.fasta\")\nsequence = FastaFile().read(sequence_path)[\"P00813\"]\nconsole.print(f\"{sequence=}\")\nconsole.print(f\"{len(sequence)=}\")\n\nsequence='MAQTPAFDKPKVELHVHLDGSIKPETILYYGRRRGIALPANTAEGLLNVIGMDKPLTLPDFLAKFDYYMPAIAGCREAIKRIAYEFVEMKAKEGVVYVEVRYSPH\nLLANSKVEPIPWNQAEGDLTPDEVVALVGQGLQEGERDFGVKARSILCCMRHQPNWSPKVVELCKKYQQQTVVAIDLAGDETIPGSSLLPGHVQAYQEAVKSGIHRTVHAGEVGS\nAEVVKEAVDILKTERLGHGYHTLEDQALYNRLRQENMHFEICPWSSYLTGAWKPDTEHAVIRLKNDQANYSLNTDDPLIFKSTLDTDYQMTKRDMGFTEEEFKRLNINAAKSSFL\nPEDEKRELLDLLYKAYGMPPSASAGQNL'\n\n\n\nlen(sequence)=363"
  },
  {
    "objectID": "index.html#creating-the-mask-libraries",
    "href": "index.html#creating-the-mask-libraries",
    "title": "Paired residue prediction dependencies in ESM2",
    "section": "Creating the mask libraries",
    "text": "Creating the mask libraries\nWe now know how to turn the model output, a logits array, into an array of amino acid probability distributions. This marks the launch point for our comparison of amino acid probability distributions for single- versus double-residue masking. We’ll do this exhaustively for our sequence of interest by establishing two different libraries: the single masks, of which there are \\(N\\), and the double masks, of which there are \\(N \\choose 2\\).\n\nfrom itertools import combinations\n\nN = len(sequence)\nsingle_masks = list(combinations(range(N), 1))\ndouble_masks = list(combinations(range(N), 2))\n\nconsole.print(f\"{single_masks[:5]=}\")\nconsole.print(f\"{double_masks[:5]=}\")\n\nassert len(single_masks) == N\nassert len(double_masks) == N * (N - 1) / 2\n\nsingle_masks[:5]=[(0,), (1,), (2,), (3,), (4,)]\n\n\n\ndouble_masks[:5]=[(0, 1), (0, 2), (0, 3), (0, 4), (0, 5)]"
  },
  {
    "objectID": "index.html#calculating-all-the-logits",
    "href": "index.html#calculating-all-the-logits",
    "title": "Paired residue prediction dependencies in ESM2",
    "section": "Calculating all the logits",
    "text": "Calculating all the logits\nThis is the compute-heavy portion of the analysis. We’re using Modal’s cloud infrastructure to distribute our computations across GPUs of varying power (from T4s to H100s). For each model in our analysis, we predict logits arrays twice: once for the single mask library, and again for the double mask library. The computation becomes increasingly demanding with larger models - ESM2-8M costs just pennies to run, while ESM2-15B costs around $30 due to its need for more powerful GPUs and smaller batch sizes. All of this behavior is encapsulated in the calculate_or_load_logits function imported below.\n\n\n\n\n\n\nNote\n\n\n\nIf you’re following along on your own computer, don’t worry - the logits are tracked in the GitHub repository, so the function call below will simply load pre-computed logits from disk without requiring Modal or GPU access.\n\n\n\n1from analysis.logits import LogitsConfig, calculate_or_load_logits\n\nconfig = LogitsConfig(\n    sequence=sequence,\n    single_masks=single_masks,\n    double_masks=double_masks,\n    single_logits_path=Path(\"output/logits_single.npz\"),\n    double_logits_path=Path(\"output/logits_double.npz\"),\n)\n\nsingle_logits, double_logits = calculate_or_load_logits(config)\n\n\n1\n\nSource code"
  },
  {
    "objectID": "index.html#conversion-to-amino-acid-probabilities",
    "href": "index.html#conversion-to-amino-acid-probabilities",
    "title": "Paired residue prediction dependencies in ESM2",
    "section": "Conversion to amino acid probabilities",
    "text": "Conversion to amino acid probabilities\nWe’ve already written a method to convert logits to amino acid probabilties, so let’s go ahead and run that conversion for all the data.\n\nsingle_probs: dict[ModelName, np.ndarray] = {}\ndouble_probs: dict[ModelName, np.ndarray] = {}\n\nfor model in ModelName:\n    single_probs[model] = get_probs_array(single_logits[model])\n    double_probs[model] = get_probs_array(double_logits[model])"
  },
  {
    "objectID": "index.html#calculating-probability-metrics-for-each-residue-pair",
    "href": "index.html#calculating-probability-metrics-for-each-residue-pair",
    "title": "Paired residue prediction dependencies in ESM2",
    "section": "Calculating probability metrics for each residue pair",
    "text": "Calculating probability metrics for each residue pair\nWith that, we finally have amino acid probability distributions for (a) each individually masked residue and (b) every pairwise combination of doubly-masked residues.\nAs a reminder, we’re interested in comparing \\(p(i | \\lbrace i \\rbrace)\\) to \\(p( i | \\lbrace i, j \\rbrace)\\) and \\(p(j | \\lbrace j \\rbrace)\\) to \\(p( j | \\lbrace i, j \\rbrace)\\). To compare the degree of similarity between probability distributions, we’re going to use Jenson-Shannon (JS) divergence3. We’ll use base-2 so that \\(0\\) corresponds to identical distributions and \\(1\\) corresponds to maximally diverged distributions.\n3 We originally planned to use Kullback-Leibler divergence, but because it’s not symmetric (i.e. \\(KL(x,y) \\ne KL(y, x)\\)), it’s not formally speaking a distance measure. JS-divergence was designed specifically to resolve this.Since we’ll have to loop through each residue pair to calculate the JS divergence, it will be useful to calculate some other metrics along the way:\n\nimport pandas as pd\n1from analysis.residue_pair_data import calculate_pairwise_data\n\npairwise_data: dict[ModelName, pd.DataFrame] = {}\n\nfor model_name in ModelName:\n    pairwise_dataframe = calculate_pairwise_data(\n        sequence=sequence,\n        single_probs=single_probs[model_name],\n        double_probs=double_probs[model_name],\n        double_masks=double_masks,\n    )\n    pairwise_data[model_name] = pairwise_dataframe\n\n\n1\n\nSource code\n\n\n\n\n/opt/miniconda3/envs/paired-token-masking/lib/python3.12/site-packages/scipy/spatial/distance.py:1271: RuntimeWarning:\n\ninvalid value encountered in sqrt\n\n\n\npairwise_data is a dictionary of dataframes, one for each model. Each dataframe holds the following metrics for each residue pair.\nCalculated metrics:\n\n\n\nField\nDescription\n\n\n\n\nposition_i\nThe \\(i^{\\text{th}}\\) residue position.\n\n\nposition_j\nThe \\(j^{\\text{th}}\\) residue position.\n\n\namino_acid_i\nThe original amino acid at the \\(i^{\\text{th}}\\) residue.\n\n\nmost_probable_i_i\nThe amino acid with the highest probability in \\(p(i \\vert \\lbrace i \\rbrace)\\).\n\n\nmost_probable_i_ij\nThe amino acid with the highest probability in \\(p( i \\vert \\lbrace i, j \\rbrace)\\).\n\n\namino_acid_j\nThe original amino acid at the \\(j^{\\text{th}}\\) residue.\n\n\nmost_probable_j_j\nThe amino acid with the highest probability in \\(p(j \\vert \\lbrace j \\rbrace)\\).\n\n\nmost_probable_j_ij\nThe amino acid with the highest probability in \\(p( j \\vert \\lbrace i, j \\rbrace)\\).\n\n\nperplex_i_i\nThe perplexity4 of \\(p(i \\vert \\lbrace i \\rbrace)\\).\n\n\nperplex_i_ij\nThe perplexity of \\(p( i \\vert \\lbrace i, j \\rbrace)\\).\n\n\nperplex_j_i\nThe perplexity of \\(p(j \\vert \\lbrace j \\rbrace)\\).\n\n\nperplex_j_ij\nThe perplexity of \\(p( j \\vert \\lbrace i, j \\rbrace)\\).\n\n\njs_div_i\nThe Jenson-Shannon divergence between \\(p(i \\vert \\lbrace i \\rbrace)\\) and \\(p( i \\vert \\lbrace i, j \\rbrace)\\).\n\n\njs_div_j\nThe Jenson-Shannon divergence between \\(p(j \\vert \\lbrace j \\rbrace)\\) and \\(p( j \\vert \\lbrace i, j \\rbrace)\\).\n\n\njs_div_avg\n(js_div_i + js_div_j) / 2\n\n\n\n4 What is perplexity? The perplexity of a probability distribution measures how uncertain or “surprised” the model is about its predictions. For amino acid predictions, a perplexity of 20 indicates maximum uncertainty (equal probability for all 20 amino acids), while a perplexity of 1 indicates complete certainty (100% probability for a single amino acid).\n$$$$\nAlso note that the perplexities in this table refer to single residues, but in the field, (pseudo-)perplexity is often used to score a model’s confidence in an entire sequence (e.g. the ESM2 paper (Lin et al., 2023)) via methods developed in Salazar et al. (2020).Let’s inspect some of the data for the largest model.\n\n\nCode\npairwise_15b = pairwise_data[ModelName.ESM2_15B]\npairwise_15b\n\n\n\n\n\n\n\n\n\nposition_i\nposition_j\namino_acid_i\nmost_probable_i_i\nmost_probable_i_ij\namino_acid_j\nmost_probable_j_j\nmost_probable_j_ij\nperplex_i_i\nperplex_i_ij\nperplex_j_j\nperplex_j_ij\njs_div_i\njs_div_j\njs_div_avg\n\n\n\n\n0\n0\n1\nM\nM\nM\nA\nA\nA\n1.874111\n1.742441\n3.191700\n2.850710\n0.084210\n0.089429\n0.086820\n\n\n1\n0\n2\nM\nM\nM\nQ\nQ\nQ\n1.874111\n1.664312\n2.018497\n1.940485\n0.045490\n0.015094\n0.030292\n\n\n2\n0\n3\nM\nM\nM\nT\nT\nT\n1.874111\n1.743261\n3.114773\n2.992301\n0.025954\n0.030157\n0.028055\n\n\n3\n0\n4\nM\nM\nM\nP\nP\nP\n1.874111\n2.434045\n1.156383\n1.146090\n0.092536\n0.005108\n0.048822\n\n\n4\n0\n5\nM\nM\nM\nA\nA\nA\n1.874111\n1.830846\n2.304582\n2.197421\n0.015346\n0.016212\n0.015779\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n131395\n362\n357\nL\nL\nL\nS\nS\nS\n2.279995\n1.893270\n2.858127\n3.379927\n0.057617\n0.092329\n0.074973\n\n\n131399\n362\n358\nL\nL\nL\nA\nA\nA\n2.279995\n1.852332\n1.807500\n1.649580\n0.063063\n0.054722\n0.058892\n\n\n131402\n362\n359\nL\nL\nL\nG\nG\nG\n2.279995\n3.412535\n4.348093\n6.531841\n0.137119\n0.191148\n0.164134\n\n\n131404\n362\n360\nL\nL\nL\nQ\nS\nS\n2.279995\n1.731056\n5.642571\n6.295696\n0.089178\n0.145349\n0.117264\n\n\n131405\n362\n361\nL\nL\nL\nN\nH\nQ\n2.279995\n2.111470\n7.408498\n10.722980\n0.088659\n0.240463\n0.164561\n\n\n\n\n131406 rows × 15 columns\n\n\n\nBefore we compare single- to double-masking results, let’s see what the reconstruction accuracy is for the protein. Relatedly, we can look at the average site perplexity to get an indication of the model’s confidence.\n\n\nCode\nimport arcadia_pycolor as apc\nimport matplotlib.pyplot as plt\n\napc.mpl.setup()\n\n\ndef get_recon_acc(df: pd.DataFrame) -&gt; float:\n    matches = df[\"most_probable_i_i\"] == df[\"amino_acid_i\"]\n    return 100 * matches.sum() / len(matches)\n\n\ndef avg_perplexity(df: pd.DataFrame) -&gt; float:\n    return df.perplex_i_i.mean()\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5.25))\n\nax1.plot(\n    [model.name for model in ModelName],\n    [get_recon_acc(pairwise_data[model]) for model in ModelName],\n    marker=\"o\",\n    linestyle=\"-\",\n    color=\"#1f77b4\",\n)\nax1.set_xlabel(\"Model\")\nax1.set_ylabel(\"Reconstruction Accuracy (%)\")\nax1.tick_params(axis=\"x\", rotation=20)\nax1.grid(True)\n\nax2.plot(\n    [model.name for model in ModelName],\n    [avg_perplexity(pairwise_data[model]) for model in ModelName],\n    marker=\"o\",\n    linestyle=\"-\",\n    color=\"#1f77b4\",\n)\nax2.set_xlabel(\"Model\")\nax2.set_ylabel(\"&lt;Perplexity&gt;\")\nax2.tick_params(axis=\"x\", rotation=20)\nax2.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Single masked token reconstruction accuracy and confidence for all ESM2 model sizes. (left) Fraction of positions that when masked, the model predicts the masked amino acid as most likely. (right) The perplexity of \\(p(i \\vert \\lbrace i \\rbrace)\\) averaged over sites."
  },
  {
    "objectID": "index.html#the-effects-of-single--versus-double-masking",
    "href": "index.html#the-effects-of-single--versus-double-masking",
    "title": "Paired residue prediction dependencies in ESM2",
    "section": "The effects of single- versus double-masking",
    "text": "The effects of single- versus double-masking\nIn the single-masking case, the model has access to all context except the masked token \\(i\\). However, in double-masking, the model loses access to both tokens \\(i\\) and \\(j\\), reducing the available context for making predictions. This reduction in context should lead to greater uncertainty in the model’s predictions. We can quantify this uncertainty using perplexity. Our hypothesis is that \\(p(i | \\lbrace i,j \\rbrace)\\) will show higher perplexity than \\(p(i | \\lbrace i \\rbrace)\\) because (1) the model has less contextual information to work with and (2) the presence of multiple masks creates more ambiguity about the relationships between the masked tokens.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfig, axes = plt.subplots(\n    1, len(ModelName), figsize=(5 * len(ModelName), 4.3), sharey=True, sharex=True\n)\n\nfor ax, model in zip(axes, ModelName, strict=True):\n    df = pairwise_data[model]\n    diff = df[\"perplex_i_ij\"] - df[\"perplex_i_i\"]\n    ax.grid(True)\n    ax.hist(diff, bins=100, edgecolor=\"black\")\n    ax.set_title(f\"{model.name}\\nAverage shift: +{diff.mean():.3f}\")\n    ax.set_xlabel(r\"$ PP_{i| \\lbrace i, j \\rbrace} - PP_{i| \\lbrace i \\rbrace} $\")\n    ax.set_yscale(\"log\")\n    if ax == axes[0]:\n        ax.set_ylabel(\"Count\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Histograms of the difference in perplexity between \\(p(i | \\lbrace i, j \\rbrace)\\) and \\(p(i | \\lbrace i \\rbrace)\\) for all residue pairs.\n\n\n\n\n\nWhile the results show that there is on average a very slight shift for higher perplexity when double-masking, the effect is very slight. These data illustrate that amino acid prediction probabilities are on average independent from each other between the single- and double-masked libraries–especially for the larger models.\nHowever, these averages might mask important position-specific dependencies. Even if masking residue \\(j\\) typically has little effect on predicting residue \\(i\\), there could be specific pairs of positions where masking \\(j\\) substantially impacts the prediction of residue \\(i\\). To identify the extent of these position pairs, we can examine a more targeted metric: for each position \\(i\\), what is the maximum increase in perplexity caused by masking any other position \\(j\\)? This analysis will help reveal whether certain amino acid positions have strong dependencies that are hidden when looking at average effects.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfig, axes = plt.subplots(\n    1, len(ModelName), figsize=(5 * len(ModelName), 4.3), sharey=True, sharex=True\n)\nfor ax, model in zip(axes, ModelName, strict=True):\n    df = pairwise_data[model]\n\n    df[\"diff\"] = df[\"perplex_i_ij\"] - df[\"perplex_i_i\"]\n    diff = df.groupby(\"position_i\")[\"diff\"].max()\n\n    ax.grid(True)\n    ax.hist(diff, bins=np.linspace(0, 16, 50), edgecolor=\"black\")\n    ax.set_title(f\"{model.name}\\nAverage max shift: +{diff.mean():.3f}\")\n    ax.set_xlabel(r\"$ \\max_j(PP_{i| \\lbrace i, j \\rbrace} - PP_{i| \\lbrace i \\rbrace}) $\")\n    ax.set_yscale(\"log\")\n    if ax == axes[0]:\n        ax.set_ylabel(\"Count\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Histograms of the max difference in perplexity between \\(p(i | \\lbrace i, j \\rbrace)\\) and \\(p(i | \\lbrace i \\rbrace)\\). Compared to Figure 2, only the position \\(j\\) that causes the largest \\(p(i | \\lbrace i, j \\rbrace)\\) for a given \\(i\\) is included.\n\n\n\n\n\nThese position-specific maximum shifts show that while random position pairs on average share little dependence, for any given position \\(i\\) there tends to exist at least one position \\(j\\) that noticeably impacts the model’s confidence in predicting \\(i\\). Even in the 15B parameter model, which has an average perplexity of less than 2, double-masking can significantly perturb perplexity. To understand the magnitude of this effect in practical terms, let’s see whether \\(p(i | \\lbrace i \\rbrace)\\) and \\(p(i | \\lbrace i,j \\rbrace)\\) ever predict different amino acids as most probable, as that would indicate positions where context truly alters the model’s understanding of the protein sequence, rather than just lowering its confidence.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\naa_changes = []\nfor model in ModelName:\n    df = pairwise_data[model]\n    df[\"matches\"] = df[\"most_probable_i_i\"] != df[\"most_probable_i_ij\"]\n    aa_change = df.groupby(\"position_i\")[\"matches\"].any().sum()\n    aa_changes.append(aa_change)\n\nplt.figure(figsize=(11, 6))\nplt.bar(\n    [model.name for model in ModelName],\n    aa_changes,\n)\nplt.title(\"Positions where double-masking changes the predicted amino acid\")\nplt.xlabel(\"Model\")\nplt.ylabel(f\"Number of positions affected\\n(total # residues = {len(sequence)})\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4: The number of positions where the most probable amino acid differs between the distributions \\(p(i | \\lbrace i \\rbrace)\\) and \\(p(i | \\lbrace i,j \\rbrace)\\).\n\n\n\n\n\nIn the largest model, more than 10% of positions have at least one partner position whose masking changes which amino acid is predicted as most likely. While this reveals clear cases of strong positional dependence, it only captures the most extreme effects - cases where context actually shifts the model’s top prediction. To capture more subtle interactions, where masking position \\(j\\) meaningfully shifts the probability distribution at position \\(i\\) without necessarily changing the most likely amino acid, we finally arrive at Jensen-Shannon (JS) divergence."
  },
  {
    "objectID": "index.html#visualizing-shannon-jensen-divergence",
    "href": "index.html#visualizing-shannon-jensen-divergence",
    "title": "Paired residue prediction dependencies in ESM2",
    "section": "Visualizing Shannon-Jensen divergence",
    "text": "Visualizing Shannon-Jensen divergence\nThe relationships between all pairs of positions can be effectively visualized using heatmaps, where each cell \\((i,j)\\) shows the JS divergence between \\(p(i|\\lbrace i \\rbrace)\\) and \\(p(i| \\lbrace i,j \\rbrace)\\).\nWith that in mind, below is a series of heatmaps for the average Jenson-Shannon divergence, with increasing model size.\n\n\nCode\n1import analysis.plotting as plotting\n\n\ndef get_js_div_matrix(df: pd.DataFrame) -&gt; np.ndarray:\n    indices = sorted(set(df[\"position_i\"]).union(df[\"position_j\"]))\n    matrix = pd.DataFrame(index=indices, columns=indices)  # type: ignore\n\n    for _, row in df.iterrows():\n        matrix.at[row.position_i, row.position_j] = row.js_div_avg\n        matrix.at[row.position_j, row.position_i] = row.js_div_avg\n\n    np.fill_diagonal(matrix.values, np.nan)\n\n    matrix_values = matrix.to_numpy()\n    return matrix_values.astype(np.float32)\n\n\nfor model in ModelName:\n    size = 700\n    fig = plotting.visualize_js_div_matrix(get_js_div_matrix(pairwise_data[model]))\n    fig.update_layout(\n        width=size,\n        height=size * 80 / 100,\n    )\n    fig.show()\n\n\n\n1\n\nSource code\n\n\n\n\n\n\nFigure 5: Heatmap of average JS divergence for each model size. Each cell is colored based on the average of \\(JS(p(i \\vert \\lbrace i \\rbrace), p(i \\vert \\lbrace i,j \\rbrace))\\) and \\(JS(p(j \\vert \\lbrace j \\rbrace), p(j \\vert \\lbrace i,j \\rbrace))\\).\n\n\n\n\n\n                                                \n\n\n(a) The ESM2_8M parameter model.\n\n\n\n\n\n\n                                                \n\n\n(b) The ESM2_35M parameter model.\n\n\n\n\n\n\n                                                \n\n\n(c) The ESM2_150M parameter model.\n\n\n\n\n\n\n                                                \n\n\n(d) The ESM2_650M parameter model.\n\n\n\n\n\n\n                                                \n\n\n(e) The ESM2_3B parameter model.\n\n\n\n\n\n\n                                                \n\n\n(f) The ESM2_15B parameter model."
  },
  {
    "objectID": "index.html#comparison-to-3d-contact-map",
    "href": "index.html#comparison-to-3d-contact-map",
    "title": "Paired residue prediction dependencies in ESM2",
    "section": "Comparison to 3D contact map",
    "text": "Comparison to 3D contact map\nA striking pattern emerges and interestingly, it is most prominently observed at the intermediately sized 150M model parameter. It looks an awful lot like a structural contact map, so let’s load up the structure, calculate the pairwise residue distances, and compare.\nWe’ll plot the JS-divergence for the 150M model on the upper left triangular region of the heatmap and the contact map on the lower right triangular region.\n\n\nCode\nimport biotite.structure as struc\nfrom biotite.structure.io import load_structure\n\n# Use alpha carbons\narray = load_structure(\"input/P00813.pdb\")\narray = array[array.atom_name == \"CA\"]\n\ndistance_map = np.zeros((len(array), len(array)))\nfor i in range(len(array)):\n    distance_map[i, :] = struc.distance(array[i], array)\n\n# Define the contact map as the inverse of the distances\ncontact_map = 1 / distance_map\n\nm150_divergence = get_js_div_matrix(pairwise_data[ModelName.ESM2_150M])\nfig = plotting.compare_to_contact_map(m150_divergence, contact_map)\nfig.show()\n\n\n/var/folders/v4/q0wyvpsj4h901lfkjcln68nw0000gn/T/ipykernel_74993/3206566214.py:13: RuntimeWarning:\n\ndivide by zero encountered in divide\n\n\n\n\n\n                                                \n\n\nFigure 6: Comparison between JS divergence and contact map. (upper-left) The heatmap values in Figure 5 for the 150M parameter model. (bottom-right) The contact map. Calculated by taking the inverse distance between alpha-carbon atoms for each pair of residues, and is expressed in units of \\(Å^{-1}\\).\n\n\n\n\nThis basically illustrates that there is some pairwise epistasis being captured, whereby masking one residue affects the probability distribution of a second masked residue, and that the strength of this effect inversely correlates with the physical distance between the residues. Given that the attention maps learned in the ESM models recapitulate protein contact maps, seeing this propagate to the logits isn’t too surprising. Although, unlike the attention maps, which recapitulate contact maps better and better with increasing model size, the correlation with contact map is strongest in intermediate model sizes.\nTo be sure, here is the same plot for the largest, most capable ESM2 model.\n\nb15_divergence = get_js_div_matrix(pairwise_data[ModelName.ESM2_15B])\nfig = plotting.compare_to_contact_map(b15_divergence, contact_map, js_div_zmax=0.05)\nfig.update_layout(\n    width=900,\n    height=701,\n)\nfig.show()\n\n\n\n                                                \n\n\nFigure 7: Comparison between JS divergence and contact map. (upper-left) The heatmap values in Figure 5 for the 15B parameter model. The contact map. Calculated by taking the inverse distance between alpha-carbon atoms for each pair of residues, and is expressed in units of \\(Å^{-1}\\)."
  },
  {
    "objectID": "index.html#conclusion",
    "href": "index.html#conclusion",
    "title": "Paired residue prediction dependencies in ESM2",
    "section": "Conclusion",
    "text": "Conclusion\nThe contrast between these two models is quite interesting. The 15B parameter model has a 90% single-mask reconstruction accuracy, whereas for the 150M parameter model the reconstruction accuracy is only 50%. Given this, we expected that the 150M parameter model would perhaps struggle to create residue pair dependencies that are biologically interpretable and that perhaps the 15B parameter model might provide more interpretable answers. However, the above results show the opposite.\nWe’re having trouble interpreting this result and we’re seeking your opinion. So our question to you is: Why do you think the contact map pattern diminishes in the JS-divergence for the larger models?"
  },
  {
    "objectID": "faq.html",
    "href": "faq.html",
    "title": "What is this?",
    "section": "",
    "text": "What is this?\nThis notebook publication (or “notebook pub”) is an experimental format we’re testing that treats a scientist’s working computational analysis as the publication itself, effectively dissolving the separation that exists between code and publication. This approach reduces publication burden by avoiding format translation, promotes sharing of early-stage work, and gives readers direct access to interact with and build upon the analysis. For more information about this experimental format, see our commentary on notebook publications."
  }
]